services:
  # CPU-only for x86_64/amd64: Linux, Windows (WSL2), Intel Macs
  local-ai-llama-cpu:
    profiles: ["cpu"]
    image: ghcr.io/ggml-org/llama.cpp:server
    platform: linux/amd64
    container_name: local-ai-llama-server
    networks:
      default:
        aliases:
          - llama-backend
    volumes:
      - ~/ai-models:/models:ro
    ports:
      - "18080:18080"
    command:
      [
        "-m", "/models/${MODEL_FILE}",
        "--host", "0.0.0.0",
        "--port", "18080"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:18080/health >/dev/null || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 180s
    restart: unless-stopped

  # NVIDIA GPU: Linux (native) or Windows (WSL2)
  local-ai-llama-nvidia:
    profiles: ["nvidia-cuda"]
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: local-ai-llama-server
    gpus: all
    networks:
      default:
        aliases:
          - llama-backend
    volumes:
      - ~/ai-models:/models:ro
    ports:
      - "18080:18080"
    command:
      [
        "-m", "/models/${MODEL_FILE}",
        "--host", "0.0.0.0",
        "--port", "18080",
        "--n-gpu-layers", "999"
      ]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:18080/health >/dev/null || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 180s
    restart: unless-stopped

  # Open WebUI for Docker-based llama.cpp (Linux, Windows, Intel Mac)
  local-ai-open-webui:
    profiles: ["cpu", "nvidia-cuda"]
    image: ghcr.io/open-webui/open-webui:main
    container_name: local-ai-open-webui
    volumes:
      - local-ai-open-webui-data:/app/backend/data
    ports:
      - "3000:8080"
    environment:
      - OPENAI_API_BASE_URL=http://llama-backend:18080/v1
      - OPENAI_API_KEY=sk-no-key-required
      - WEBUI_AUTH=false
    restart: unless-stopped

  # Open WebUI for Apple Silicon (connects to native llama-server on host)
  local-ai-open-webui-mac:
    profiles: ["mac"]
    image: ghcr.io/open-webui/open-webui:main
    container_name: local-ai-open-webui
    volumes:
      - local-ai-open-webui-data:/app/backend/data
    ports:
      - "3000:8080"
    environment:
      - OPENAI_API_BASE_URL=http://host.docker.internal:18080/v1
      - OPENAI_API_KEY=sk-no-key-required
      - WEBUI_AUTH=false
    restart: unless-stopped

volumes:
  local-ai-open-webui-data:
